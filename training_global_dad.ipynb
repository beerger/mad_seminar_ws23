{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global-Net and DAD-head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run these cells only when in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/beerger/mad_seminar_ws23.git\n",
    "# Move all content to the current directory\n",
    "!mv ./mad_seminar_ws23/* ./\n",
    "# Remove the empty directory\n",
    "!rm -rf mad_seminar_ws23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages\n",
    "!pip install pytorch_lightning --quiet\n",
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Global-Net and DAD-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "from model.local_net import LocalNet\n",
    "from model.dad import DADHead\n",
    "from model.iad import iad_head\n",
    "from model.global_net import GlobalNet\n",
    "from data_loader.joint_training_data_loader import JointTrainingDataModule\n",
    "from model.joint_training_module import JointGlobalDADTrainingModule\n",
    "\n",
    "# autoreload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configs/global_dad_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount current Colab session to Google Drive (training/val images are stored here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will provide you with an authentication link\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy zipped file of zipper dataset from Google Drive to current Colab session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"/content/drive/MyDrive/AnomalyDetection/Datasets/MVTec/zipper.tar.xz\" \"/content/\"\n",
    "# Unzip it\n",
    "!tar -xf /content/zipper.tar.xz -C /content/\n",
    "# Remove zip file\n",
    "!rm -rf zipper.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get paths to training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory you want to list\n",
    "train_directory = '/content/zipper/train/good/'\n",
    "\n",
    "# Get a list of all files in the train_directory\n",
    "file_list = [train_directory + f for f in os.listdir(train_directory) if os.path.isfile(os.path.join(train_directory, f))]\n",
    "\n",
    "assert len(file_list) == 240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming file_paths is your list of 240 image file paths\n",
    "train_image_paths, val_image_paths = train_test_split(file_list, test_size=0.1, random_state=config['seed'])  # Adjust test_size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data module for joint training on MVTec AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = JointTrainingDataModule(\n",
    "    train_image_paths, \n",
    "    val_image_paths, \n",
    "    batch_size=config['batch_size'], \n",
    "    num_workers=2, \n",
    "    caching_strategy='at-init'\n",
    ")\n",
    "\n",
    "print(f\"Number of training images: {len(train_image_paths)}\")\n",
    "print(f\"Number of validation images: {len(val_image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize patches, images, binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure batch_size in data_module is equal to BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE=4\n",
    "\n",
    "# Reverse the normalization process done by ImageNetDataModule\n",
    "# to avoid the following error:\n",
    "# WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverses the normalization in-place\n",
    "    return tensor.clamp(0, 1)  # Ensures the pixel values are within [0, 1]\n",
    "\n",
    "\n",
    "# Retrieve one batch of images\n",
    "I, patches, binary_masks, labels = next(iter(data_module.train_dataloader()))\n",
    "\n",
    "# Denormalize the patches for visualization\n",
    "patches = denormalize(patches)\n",
    "\n",
    "fig1, ax1 = plt.subplots(1, BATCH_SIZE, figsize=(20, 8))\n",
    "\n",
    "# Plotting patches\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patches[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax1[i].imshow(image.cpu().numpy())\n",
    "    ax1[i].axis('on')\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, BATCH_SIZE, figsize=(20, 8))\n",
    "\n",
    "# Plotting images\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = I[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax2[i].imshow(image.cpu().numpy())\n",
    "    ax2[i].axis('on')\n",
    "\n",
    "fig3, ax3 = plt.subplots(1, BATCH_SIZE, figsize=(20, 8))\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Squeeze the tensor to 2D [H, W] if it's 3D [1, H, W]\n",
    "    mask = binary_masks[i].squeeze()\n",
    "\n",
    "    # Display the mask\n",
    "    ax3[i].imshow(mask.cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    ax3[i].axis('on')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "for i, mask in enumerate(binary_masks):\n",
    "    print(f\"Size of binary mask {i}: {mask.size()}\")\n",
    "    print(f\"Mask {i} values: Unique: {torch.unique(mask)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all models for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is either 'cuda' if a GPU is available, otherwise 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load fine-tuned local_net from Google Drive\n",
    "local_net = LocalNet().to(device)\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/Trained Models/local_net_finetuned.pth', map_location=device)\n",
    "# Update the local_net model's state dictionary\n",
    "local_net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "global_net = GlobalNet().to(device)\n",
    "dad_head = DADHead().to(device)\n",
    "\n",
    "joint_train_module = JointGlobalDADTrainingModule(\n",
    "    config, \n",
    "    local_net=local_net, \n",
    "    global_net=global_net, \n",
    "    dad_head=dad_head,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given by paper is batch size of 64 for 50k iterations\n",
    "# Need to calculate max_epochs\n",
    "total_iterations = config['iterations']\n",
    "batch_size = config['batch_size']\n",
    "num_training_images = len(train_image_paths)\n",
    "# Calculate max_epochs\n",
    "max_epochs = total_iterations / (num_training_images / batch_size)\n",
    "max_epochs = int(max_epochs) + (max_epochs % 1 > 0)  # round up if not an integer\n",
    "print(f\"Calculated max_epochs: {max_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create callbacks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Checkpoints\",  # Path where checkpoints will be saved\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",  # Filename template\n",
    "    monitor=\"val_loss\",  # Metric to monitor for saving\n",
    "    every_n_epochs=1,  # Save every epoch\n",
    "    save_weights_only=True,  # If True, save only the model weights, not the full model\n",
    "    save_top_k=3,  # Save the top 3 checkpoints based on val_loss\n",
    "    save_last=True,  # Also save the last checkpoint to resume training later\n",
    "    verbose=True  # If True, print a message to stdout for each save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup trainer from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change <CHECK_POINT.ckpt> to a valid checkpoint file located in\n",
    "# /content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Checkpoints\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    resume_from_checkpoint=\"/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Checkpoints/<CHECK_POINT.ckpt>\",\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run joint training (and save trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(joint_train_module, datamodule=data_module)\n",
    "torch.save(global_net.state_dict(), '/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Trained Models/global_net.pth')\n",
    "torch.save(dad_head.state_dict(), '/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Trained Models/dad_head.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
