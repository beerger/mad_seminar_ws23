{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-Level Localization on MVTec AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run these cells only when in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/beerger/mad_seminar_ws23.git\n",
    "# Move all content to the current directory\n",
    "!mv ./mad_seminar_ws23/* ./\n",
    "# Remove the empty directory\n",
    "!rm -rf mad_seminar_ws23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages\n",
    "!pip install pytorch_lightning --quiet\n",
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Local-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "from model.local_net import LocalNet\n",
    "from model.model_utils import load_resnet_18_teacher_model\n",
    "from model.student_training_module import StudentTrainingModule\n",
    "from image_net_data_loader import ImageNetDataModule\n",
    "from model.one_layer_decoder import OneLayerDecoder\n",
    "\n",
    "# autoreload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-training\n",
    "\n",
    "Until next numbered step the following code blocks will be part of the training of Local-Net. This is refered to as the pre-training of the framework, since the Local-Nets parameters will be fixed during training of Global-Net and DAD-head. This consists of two major steps:\n",
    "\n",
    "* **Distillation**: on ImageNet, where the teacher network is pretrained ResNet-18.\n",
    "* **Fine-tuning**: on some certain category of MVTec AD\n",
    "\n",
    "Pre-processing in accordance (*) to ResNet-18 documentation\n",
    "@ https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html\n",
    "\n",
    "(*) According to documentation it's first resized to 256x256 then\n",
    "center cropped to 224x224. This step has been skipped, \n",
    "and it is instead resized directly to 224x224. \n",
    "The reason for this is because the input patch size to Local-Net is 33x33\n",
    "and ResNet-18 has input size 224x224, \n",
    "meaning that resizing to 256x256 and then cropping to desired size (224x224)\n",
    "would result in altering the original spatial relationships and scale of the features of the image.\n",
    "This could potentially lead to a mismatch when comparing features extracted from the resized and \n",
    "cropped image by ResNet-18 with those extracted from the original 33x33 image by Local-Net.\n",
    "\n",
    "Pre-processing the data for distillation consists of 4 major steps:\n",
    "\n",
    "1. Load images from Image-Net\n",
    "2. Resize images to 256 x 256 (done by most applications, and mentioned in Supplementary Material)\n",
    "3. Extract 33 x 33 patches from each resized image\n",
    "4. Create two separate transform pipelines\n",
    "    * For Local-Net: Convert the 33x33 patches to PyTorch tensors and normalize them\n",
    "    * For ResNet-18 (teacher model): Resize the 33x33 patches to 224x224, then convert to tensors and normalize\n",
    "\n",
    "<span style=\"color:yellow\"> **Note**: All pre-processing is done by *ImageNetDataModule* provided in *image_net_data_loader.py* </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configs/local_net_distillation.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount current Colab session to Google Drive (training/val images are stored here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will provide you with an authentication link\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy data from Google Drive to Colab VM's local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy training and validation zips from Google Drive\n",
    "# Takes ~ 15 min\n",
    "!cp \"/content/drive/MyDrive/AnomalyDetection/Datasets/ImageNet/train.zip\" \"/content/\"\n",
    "!cp \"/content/drive/MyDrive/AnomalyDetection/Datasets/ImageNet/val.zip\" \"/content/\"\n",
    "\n",
    "# Unzip\n",
    "# Takes ~ 7 min\n",
    "!unzip \"/content/train.zip\" -d \"/content/train\"\n",
    "!unzip \"/content/val.zip\" -d \"/content/val\"\n",
    "\n",
    "# Delete the zip files to free up space\n",
    "!rm \"/content/train.zip\"\n",
    "!rm \"/content/val.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data module for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_paths(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_image_paths = load_image_paths('/content/drive/MyDrive/AnomalyDetection/Datasets/ImageNet/train_image_local_paths.json')\n",
    "val_image_paths = load_image_paths('/content/drive/MyDrive/AnomalyDetection/Datasets/ImageNet/val_image_local_paths.json')\n",
    "\n",
    "data_module = ImageNetDataModule(\n",
    "    train_image_paths, \n",
    "    val_image_paths, \n",
    "    batch_size=config['batch_size'], \n",
    "    num_workers=4, \n",
    "    caching_strategy='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot patches for Local-Net and ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure batch_size in data_module is equal to BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE=config['batch_size']\n",
    "\n",
    "# Reverse the normalization process done by ImageNetDataModule\n",
    "# to avoid the following error:\n",
    "# WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverses the normalization in-place\n",
    "    return tensor.clamp(0, 1)  # Ensures the pixel values are within [0, 1]\n",
    "\n",
    "\n",
    "# Retrieve one batch of images\n",
    "patch_local, patch_resnet = next(iter(data_module.train_dataloader()))\n",
    "\n",
    "# Denormalize the patches for visualization\n",
    "patch_local = denormalize(patch_local)\n",
    "patch_resnet = denormalize(patch_resnet)\n",
    "\n",
    "fig, ax = plt.subplots(2, BATCH_SIZE, figsize=(20, 8))  # 2 rows, BATCH_SIZE columns\n",
    "\n",
    "# Plotting patch_local images in the first row\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patch_local[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax[0, i].imshow(image.cpu().numpy())\n",
    "    ax[0, i].axis('off')\n",
    "\n",
    "# Plotting patch_resnet images in the second row\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patch_resnet[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax[1, i].imshow(image.cpu().numpy())\n",
    "    ax[1, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all models for distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is either 'cuda' if a GPU is available, otherwise 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_net = LocalNet(config).to(device)\n",
    "resnet_18 = load_resnet_18_teacher_model('resnet18-5c106cde.pth', device)\n",
    "decoder = OneLayerDecoder(config['local_net_output_dimensions'], \n",
    "                          config['resnet_output_dimensions']).to(device)\n",
    "\n",
    "student_train_module = StudentTrainingModule(\n",
    "    config, \n",
    "    student_model=local_net, \n",
    "    teacher_model=resnet_18, \n",
    "    decoder=decoder, \n",
    "    mode='distillation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given by paper is batch size of 64 for 50k iterations\n",
    "# Need to calculate max_epochs\n",
    "total_iterations = config['iterations']\n",
    "batch_size = config['batch_size']\n",
    "num_training_images = len(train_image_paths)\n",
    "# Calculate max_epochs\n",
    "max_epochs = total_iterations / (num_training_images / batch_size)\n",
    "max_epochs = int(max_epochs) + (max_epochs % 1 > 0)  # round up if not an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete callbacks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/content/drive/MyDrive/AnomalyDetection/LocalNet/Checkpoints\",  # Path where checkpoints will be saved\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",  # Filename template\n",
    "    monitor=\"val_loss\",  # Metric to monitor for saving\n",
    "    every_n_epochs=1,  # Save every epoch\n",
    "    save_weights_only=True  # If True, save only the model weights, not the full model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup trainer from checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change <CHECK_POINT.ckpt> to a valid checkpoint file located in\n",
    "# /content/drive/MyDrive/AnomalyDetection/LocalNet/Checkpoints/\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    resume_from_checkpoint=\"/content/drive/MyDrive/AnomalyDetection/LocalNet/Checkpoints/<CHECK_POINT.ckpt>\",\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(student_train_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(local_net.state_dict(), '/content/drive/MyDrive/AnomalyDetection/LocalNet/local_net_distilled.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
