{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-Level Localization on MVTec AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run these cells only when in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/beerger/mad_seminar_ws23.git\n",
    "# Move all content to the current directory\n",
    "!mv ./mad_seminar_ws23/* ./\n",
    "# Remove the empty directory\n",
    "!rm -rf mad_seminar_ws23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages\n",
    "!pip install pytorch_lightning --quiet\n",
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for Local-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "\n",
    "from model.local_net import LocalNet\n",
    "from model.model_utils import load_resnet_18_teacher_model\n",
    "from model.student_training_module import StudentTrainingModule\n",
    "from image_net_data_loader import ImageNetDataModule\n",
    "\n",
    "# autoreload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-training\n",
    "\n",
    "Until next numbered step the following code blocks will be part of the training of Local-Net. This is refered to as the pre-training of the framework, since the Local-Nets parameters will be fixed during training of Global-Net and DAD-head. This consists of two major steps:\n",
    "\n",
    "* **Distillation**: on ImageNet, where the teacher network is pretrained ResNet-18.\n",
    "* **Fine-tuning**: on some certain category of MVTec AD\n",
    "\n",
    "Pre-processing in accordance (*) to ResNet-18 documentation\n",
    "@ https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html\n",
    "\n",
    "(*) According to documentation it's first resized to 256x256 then\n",
    "center cropped to 224x224. This step has been skipped, \n",
    "and it is instead resized directly to 224x224. \n",
    "The reason for this is because the input patch size to Local-Net is 33x33\n",
    "and ResNet-18 has input size 224x224, \n",
    "meaning that resizing to 256x256 and then cropping to desired size (224x224)\n",
    "would result in altering the original spatial relationships and scale of the features of the image.\n",
    "This could potentially lead to a mismatch when comparing features extracted from the resized and \n",
    "cropped image by ResNet-18 with those extracted from the original 33x33 image by Local-Net.\n",
    "\n",
    "Pre-processing the data for distillation consists of 4 major steps:\n",
    "\n",
    "1. Load images from Image-Net\n",
    "2. Resize images to 256 x 256 (done by most applications, and mentioned in Supplementary Material)\n",
    "3. Extract 33 x 33 patches from each resized image\n",
    "4. Create two separate transform pipelines\n",
    "    * For Local-Net: Convert the 33x33 patches to PyTorch tensors and normalize them\n",
    "    * For ResNet-18 (teacher model): Resize the 33x33 patches to 224x224, then convert to tensors and normalize\n",
    "\n",
    "<span style=\"color:yellow\"> **Note**: All pre-processing is done by *ImageNetDataModule* provided in *image_net_data_loader.py* </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configs/local_net_distillation.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports to use ImageNet from Hugging Face\n",
    "!pip install -U \"huggingface_hub[cli]\"\n",
    "!pip install datasets\n",
    "# To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount Google Drive in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "# Will provide you with an authentication link\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Cache Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set a path on your Google Drive for the cache\n",
    "cache_dir = '/content/drive/MyDrive/ImageNet'\n",
    "\n",
    "# Make sure the cache directory exists\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset with specified cache directory\n",
    "dataset = load_dataset('imagenet-1k', cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training data from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_net_train_data_module = ImageNetDataModule(dataset, config['batch_size'])\n",
    "\n",
    "batch = next(iter(image_net_train_data_module.train_dataloader()))\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Batch min: {batch.min()}\")\n",
    "print(f\"Batch max: {batch.max()}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(batch[i].squeeze(), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Distillation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
