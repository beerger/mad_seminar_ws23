{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run these cells only when in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/beerger/mad_seminar_ws23.git\n",
    "# Move all content to the current directory\n",
    "!mv ./mad_seminar_ws23/* ./\n",
    "# Remove the empty directory\n",
    "!rm -rf mad_seminar_ws23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the data\n",
    "!wget https://syncandshare.lrz.de/dl/fiH6r4B6WyzAaxZXTEAYCE/data.zip\n",
    "# # Extract the data\n",
    "!unzip -q ./data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages\n",
    "!pip install pytorch_lightning --quiet\n",
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from model.local_net import LocalNet\n",
    "from model.model_utils import load_resnet_18_teacher_model\n",
    "from model.student_training_module import StudentTrainingModule\n",
    "from data_loader.mvtec_data_loader import MVTecDataModule\n",
    "from model.one_layer_decoder import OneLayerDecoder\n",
    "\n",
    "# autoreload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configs/local_net_fine_tune.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount current Colab session to Google Drive (training/val images are stored here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will provide you with an authentication link\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get file paths to train/val images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dir = \"./data/splits\"\n",
    "\n",
    "train_csv_ixi = os.path.join(split_dir, 'ixi_normal_train.csv')\n",
    "train_csv_fastMRI = os.path.join(split_dir, 'normal_train.csv')\n",
    "val_csv = os.path.join(split_dir, 'normal_val.csv')\n",
    "# Load csv files\n",
    "train_files_ixi = pd.read_csv(train_csv_ixi)['filename'].tolist()\n",
    "train_files_fastMRI = pd.read_csv(train_csv_fastMRI)['filename'].tolist()\n",
    "val_files = pd.read_csv(val_csv)['filename'].tolist()\n",
    "# Combine files\n",
    "train_image_paths = train_files_ixi + train_files_fastMRI\n",
    "val_image_paths = val_files\n",
    "\n",
    "print(f\"Using {len(train_files_ixi)} IXI images \"\n",
    "      f\"and {len(train_files_fastMRI)} fastMRI images for training. \"\n",
    "      f\"Using {len(val_files)} images for validation.\")\n",
    "\n",
    "# Ensure that it's file paths\n",
    "print(train_image_paths)\n",
    "print(val_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MVTecDataModule(\n",
    "    train_image_paths, \n",
    "    val_image_paths, \n",
    "    batch_size=config['batch_size'], \n",
    "    num_workers=4, \n",
    "    caching_strategy='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure batch_size in data_module is equal to BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE=config['batch_size']\n",
    "\n",
    "# Reverse the normalization process done by ImageNetDataModule\n",
    "# to avoid the following error:\n",
    "# WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverses the normalization in-place\n",
    "    return tensor.clamp(0, 1)  # Ensures the pixel values are within [0, 1]\n",
    "\n",
    "\n",
    "# Retrieve one batch of images\n",
    "patch_local, patch_resnet = next(iter(data_module.train_dataloader()))\n",
    "\n",
    "# Denormalize the patches for visualization\n",
    "patch_local = denormalize(patch_local)\n",
    "patch_resnet = denormalize(patch_resnet)\n",
    "\n",
    "fig, ax = plt.subplots(2, BATCH_SIZE, figsize=(20, 8))  # 2 rows, BATCH_SIZE columns\n",
    "\n",
    "# Plotting patch_local images in the first row\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patch_local[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax[0, i].imshow(image.cpu().numpy())\n",
    "    ax[0, i].axis('off')\n",
    "\n",
    "# Plotting patch_resnet images in the second row\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patch_resnet[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax[1, i].imshow(image.cpu().numpy())\n",
    "    ax[1, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all models for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is either 'cuda' if a GPU is available, otherwise 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load distilled local_net from Google Drive\n",
    "local_net = LocalNet().to(device)\n",
    "# Load the state dictionary from the saved file\n",
    "local_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Distillation/Trained Models/V2/local_net_distilled_v2.pth', map_location=device)\n",
    "# Update the local_net model's state dictionary\n",
    "local_net.load_state_dict(local_state_dict)\n",
    "\n",
    "resnet_18 = load_resnet_18_teacher_model('resnet18-5c106cde.pth', device)\n",
    "decoder = OneLayerDecoder(config['local_net_output_dimensions'], \n",
    "                          config['resnet_output_dimensions']).to(device)\n",
    "\n",
    "decoder_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Distillation/Trained Models/V2/decoder_v2.pth')\n",
    "\n",
    "decoder.load_state_dict(decoder_state_dict)\n",
    "\n",
    "student_train_module = StudentTrainingModule(\n",
    "    config, \n",
    "    student_model=local_net, \n",
    "    teacher_model=resnet_18, \n",
    "    decoder=decoder, \n",
    "    mode='finetuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given by paper is batch size of 64 for 50k iterations\n",
    "# Need to calculate max_epochs\n",
    "total_iterations = config['iterations']\n",
    "batch_size = config['batch_size']\n",
    "num_training_images = len(train_image_paths)\n",
    "# Calculate max_epochs\n",
    "max_epochs = total_iterations / (num_training_images / batch_size)\n",
    "max_epochs = int(max_epochs) + (max_epochs % 1 > 0)  # round up if not an integer\n",
    "print(f\"Calculated max_epochs: {max_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create callbacks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/X-ray/Checkpoints/V1\",  # Path where checkpoints will be saved\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",  # Filename template\n",
    "    monitor=\"val_loss\",  # Metric to monitor for saving\n",
    "    every_n_epochs=1,  # Save every epoch\n",
    "    save_weights_only=True,  # If True, save only the model weights, not the full model\n",
    "    save_top_k=3,  # Save the top 3 checkpoints based on val_loss\n",
    "    save_last=True,  # Also save the last checkpoint to resume training later\n",
    "    verbose=True  # If True, print a message to stdout for each save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(student_train_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model by first loading given checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_net = LocalNet()\n",
    "resnet_18 = load_resnet_18_teacher_model('resnet18-5c106cde.pth', device)\n",
    "decoder = OneLayerDecoder(128, 512)\n",
    "\n",
    "student_train_module = StudentTrainingModule(\n",
    "    config, \n",
    "    student_model=local_net, \n",
    "    teacher_model=resnet_18, \n",
    "    decoder=decoder, \n",
    "    mode='finetuning'\n",
    ")\n",
    "\n",
    "# Replace with correct checkpoint path\n",
    "checkpoint = torch.load(\"/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/Checkpoints/V4/epoch=3535-val_loss=1890.43.ckpt\")\n",
    "student_train_module.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "local_net = student_train_module.student_model\n",
    "\n",
    "# Save the state dictionaries of the individual models\n",
    "torch.save(local_net.state_dict(), '/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/Trained Models/V4/local_net_finetuned_v4.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
