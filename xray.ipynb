{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Run these cells only when in Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/beerger/mad_seminar_ws23.git\n",
    "# Move all content to the current directory\n",
    "!mv ./mad_seminar_ws23/* ./\n",
    "# Remove the empty directory\n",
    "!rm -rf mad_seminar_ws23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the data\n",
    "!wget https://syncandshare.lrz.de/dl/fiH6r4B6WyzAaxZXTEAYCE/data.zip\n",
    "# # Extract the data\n",
    "!unzip -q ./data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages\n",
    "!pip install pytorch_lightning --quiet\n",
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from model.local_net import LocalNet\n",
    "from model.model_utils import load_resnet_18_teacher_model\n",
    "from model.student_training_module import StudentTrainingModule\n",
    "from data_loader.mvtec_data_loader import MVTecDataModule\n",
    "from model.one_layer_decoder import OneLayerDecoder\n",
    "from model.dad import DADHead\n",
    "from model.iad import iad_head\n",
    "from model.global_net import GlobalNet\n",
    "from data_loader.joint_training_data_loader import JointTrainingDataModule\n",
    "from model.joint_training_module import JointGlobalDADTrainingModule\n",
    "from data_loader.mvtec_inference_data_loader import MVTecInferenceDataModule\n",
    "from model.anomaly_detector import AnomalyDetector\n",
    "\n",
    "# autoreload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train/val image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dir = \"./data/splits\"\n",
    "\n",
    "train_csv_ixi = os.path.join(split_dir, 'ixi_normal_train.csv')\n",
    "train_csv_fastMRI = os.path.join(split_dir, 'normal_train.csv')\n",
    "val_csv = os.path.join(split_dir, 'normal_val.csv')\n",
    "# Load csv files\n",
    "train_files_ixi = pd.read_csv(train_csv_ixi)['filename'].tolist()\n",
    "train_files_fastMRI = pd.read_csv(train_csv_fastMRI)['filename'].tolist()\n",
    "val_files = pd.read_csv(val_csv)['filename'].tolist()\n",
    "# Combine files\n",
    "train_file_paths = train_files_ixi + train_files_fastMRI\n",
    "val_file_paths = val_files\n",
    "\n",
    "print(f\"Using {len(train_files_ixi)} IXI images \"\n",
    "      f\"and {len(train_files_fastMRI)} fastMRI images for training. \"\n",
    "      f\"Using {len(val_files)} images for validation.\")\n",
    "\n",
    "# Ensure that it's file paths\n",
    "print(train_file_paths)\n",
    "print(val_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory you want to list\n",
    "train_directory = '/content/zipper/test/'\n",
    "\n",
    "test_file_paths = []\n",
    "for root, dirs, files in os.walk(train_directory, topdown=False):\n",
    "   for name in files:\n",
    "      test_file_paths.append(os.path.join(root, name))\n",
    "\n",
    "assert len(test_file_paths) == 151\n",
    "print(test_file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Local-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configs/local_net_fine_tune.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount current Colab session to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will provide you with an authentication link\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MVTecDataModule(\n",
    "    train_image_paths, \n",
    "    val_image_paths, \n",
    "    batch_size=config['batch_size'], \n",
    "    num_workers=4, \n",
    "    caching_strategy='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure batch_size in data_module is equal to BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE=config['batch_size']\n",
    "\n",
    "# Reverse the normalization process done by ImageNetDataModule\n",
    "# to avoid the following error:\n",
    "# WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverses the normalization in-place\n",
    "    return tensor.clamp(0, 1)  # Ensures the pixel values are within [0, 1]\n",
    "\n",
    "\n",
    "# Retrieve one batch of images\n",
    "patch_local, patch_resnet = next(iter(data_module.train_dataloader()))\n",
    "\n",
    "# Denormalize the patches for visualization\n",
    "patch_local = denormalize(patch_local)\n",
    "patch_resnet = denormalize(patch_resnet)\n",
    "\n",
    "fig, ax = plt.subplots(2, BATCH_SIZE, figsize=(20, 8))  # 2 rows, BATCH_SIZE columns\n",
    "\n",
    "# Plotting patch_local images in the first row\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patch_local[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax[0, i].imshow(image.cpu().numpy())\n",
    "    ax[0, i].axis('off')\n",
    "\n",
    "# Plotting patch_resnet images in the second row\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patch_resnet[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax[1, i].imshow(image.cpu().numpy())\n",
    "    ax[1, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all models for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is either 'cuda' if a GPU is available, otherwise 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load distilled local_net from Google Drive\n",
    "local_net = LocalNet().to(device)\n",
    "# Load the state dictionary from the saved file\n",
    "local_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Distillation/Trained Models/V2/local_net_distilled_v2.pth', map_location=device)\n",
    "# Update the local_net model's state dictionary\n",
    "local_net.load_state_dict(local_state_dict)\n",
    "\n",
    "resnet_18 = load_resnet_18_teacher_model('resnet18-5c106cde.pth', device)\n",
    "decoder = OneLayerDecoder(config['local_net_output_dimensions'], \n",
    "                          config['resnet_output_dimensions']).to(device)\n",
    "\n",
    "decoder_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Distillation/Trained Models/V2/decoder_v2.pth')\n",
    "\n",
    "decoder.load_state_dict(decoder_state_dict)\n",
    "\n",
    "student_train_module = StudentTrainingModule(\n",
    "    config, \n",
    "    student_model=local_net, \n",
    "    teacher_model=resnet_18, \n",
    "    decoder=decoder, \n",
    "    mode='finetuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given by paper is batch size of 64 for 50k iterations\n",
    "# Need to calculate max_epochs\n",
    "total_iterations = config['iterations']\n",
    "batch_size = config['batch_size']\n",
    "num_training_images = len(train_image_paths)\n",
    "# Calculate max_epochs\n",
    "max_epochs = total_iterations / (num_training_images / batch_size)\n",
    "max_epochs = int(max_epochs) + (max_epochs % 1 > 0)  # round up if not an integer\n",
    "print(f\"Calculated max_epochs: {max_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create callbacks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/X-ray/Checkpoints/V1\",  # Path where checkpoints will be saved\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",  # Filename template\n",
    "    monitor=\"val_loss\",  # Metric to monitor for saving\n",
    "    every_n_epochs=1,  # Save every epoch\n",
    "    save_weights_only=True,  # If True, save only the model weights, not the full model\n",
    "    save_top_k=3,  # Save the top 3 checkpoints based on val_loss\n",
    "    save_last=True,  # Also save the last checkpoint to resume training later\n",
    "    verbose=True  # If True, print a message to stdout for each save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(student_train_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model by first loading given checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_net = LocalNet()\n",
    "resnet_18 = load_resnet_18_teacher_model('resnet18-5c106cde.pth', device)\n",
    "decoder = OneLayerDecoder(128, 512)\n",
    "\n",
    "student_train_module = StudentTrainingModule(\n",
    "    config, \n",
    "    student_model=local_net, \n",
    "    teacher_model=resnet_18, \n",
    "    decoder=decoder, \n",
    "    mode='finetuning'\n",
    ")\n",
    "\n",
    "# Replace with correct checkpoint path\n",
    "checkpoint = torch.load(\"/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/Checkpoints/V4/epoch=3535-val_loss=1890.43.ckpt\")\n",
    "student_train_module.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "local_net = student_train_module.student_model\n",
    "\n",
    "# Save the state dictionaries of the individual models\n",
    "torch.save(local_net.state_dict(), '/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/Trained Models/V4/local_net_finetuned_v4.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Global-Net and DAD-head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./configs/global_dad_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount current Colab session to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will provide you with an authentication link\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = JointTrainingDataModule(\n",
    "    train_image_paths, \n",
    "    val_image_paths, \n",
    "    batch_size=config['batch_size'], \n",
    "    num_workers=2, \n",
    "    caching_strategy='at-init'\n",
    ")\n",
    "\n",
    "print(f\"Number of training images: {len(train_image_paths)}\")\n",
    "print(f\"Number of validation images: {len(val_image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize patches, images, binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure batch_size in data_module is equal to BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE=4\n",
    "\n",
    "# Reverse the normalization process done by ImageNetDataModule\n",
    "# to avoid the following error:\n",
    "# WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverses the normalization in-place\n",
    "    return tensor.clamp(0, 1)  # Ensures the pixel values are within [0, 1]\n",
    "\n",
    "\n",
    "# Retrieve one batch of images\n",
    "I, patches, binary_masks, labels = next(iter(data_module.train_dataloader()))\n",
    "\n",
    "# Denormalize the patches for visualization\n",
    "patches = denormalize(patches)\n",
    "\n",
    "fig1, ax1 = plt.subplots(1, BATCH_SIZE, figsize=(20, 8))\n",
    "\n",
    "# Plotting patches\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = patches[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax1[i].imshow(image.cpu().numpy())\n",
    "    ax1[i].axis('on')\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, BATCH_SIZE, figsize=(20, 8))\n",
    "\n",
    "# Plotting images\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Permute the tensor to the format (H, W, C)\n",
    "    image = I[i].permute(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    ax2[i].imshow(image.cpu().numpy())\n",
    "    ax2[i].axis('on')\n",
    "\n",
    "fig3, ax3 = plt.subplots(1, BATCH_SIZE, figsize=(20, 8))\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Squeeze the tensor to 2D [H, W] if it's 3D [1, H, W]\n",
    "    mask = binary_masks[i].squeeze()\n",
    "\n",
    "    # Display the mask\n",
    "    ax3[i].imshow(mask.cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    ax3[i].axis('on')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all models for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is either 'cuda' if a GPU is available, otherwise 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load fine-tuned local_net from Google Drive\n",
    "local_net = LocalNet().to(device)\n",
    "# Load the state dictionary from the saved file\n",
    "state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/X-ray/Trained Models/V1/local_net_finetuned_xray_v1.pth', map_location=device)\n",
    "# Update the local_net model's state dictionary\n",
    "local_net.load_state_dict(state_dict)\n",
    "\n",
    "global_net = GlobalNet().to(device)\n",
    "dad_head = DADHead().to(device)\n",
    "\n",
    "joint_train_module = JointGlobalDADTrainingModule(\n",
    "    config, \n",
    "    local_net=local_net, \n",
    "    global_net=global_net, \n",
    "    dad_head=dad_head,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given by paper is batch size of 64 for 50k iterations\n",
    "# Need to calculate max_epochs\n",
    "total_iterations = config['iterations']\n",
    "batch_size = config['batch_size']\n",
    "num_training_images = len(train_image_paths)\n",
    "# Calculate max_epochs\n",
    "max_epochs = total_iterations / (num_training_images / batch_size)\n",
    "max_epochs = int(max_epochs) + (max_epochs % 1 > 0)  # round up if not an integer\n",
    "print(f\"Calculated max_epochs: {max_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create callbacks for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Setup the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Checkpoints/V3\",  # Path where checkpoints will be saved\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",  # Filename template\n",
    "    monitor=\"val_loss\",  # Metric to monitor for saving\n",
    "    every_n_epochs=1,  # Save every epoch\n",
    "    save_weights_only=True,  # If True, save only the model weights, not the full model\n",
    "    save_top_k=3,  # Save the top 3 checkpoints based on val_loss\n",
    "    save_last=True,  # Also save the last checkpoint to resume training later\n",
    "    verbose=True  # If True, print a message to stdout for each save\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=[\n",
    "        pl.loggers.TensorBoardLogger(save_dir='./')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run joint training (and save trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(joint_train_module, datamodule=data_module)\n",
    "torch.save(global_net.state_dict(), '/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Trained Models/V3/global_net_v3.pth')\n",
    "torch.save(dad_head.state_dict(), '/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/Trained Models/V3/dad_head_v3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load a certain checkpoint and save the models from that checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load fine-tuned local_net from Google Drive\n",
    "local_net = LocalNet().to(device)\n",
    "global_net = GlobalNet().to(device)\n",
    "dad_head = DADHead().to(device)\n",
    "\n",
    "joint_train_module = JointGlobalDADTrainingModule(\n",
    "    config, \n",
    "    local_net=local_net, \n",
    "    global_net=global_net, \n",
    "    dad_head=dad_head,  \n",
    ")\n",
    "\n",
    "# Step 2: Load the checkpoint\n",
    "# Replace '/path/to/checkpoint.ckpt' with the path to your checkpoint file\n",
    "checkpoint = torch.load(\"/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/X-ray/Checkpoints/V1/epoch=5481-val_loss=0.09021.ckpt\", map_location=device)\n",
    "joint_train_module.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Step 3: Extract and save the individual models\n",
    "# Assuming `global_net` and `dad_head` are attributes of your joint module\n",
    "global_net = joint_train_module.global_net\n",
    "dad_head = joint_train_module.dad_head\n",
    "\n",
    "# Save the state dictionaries of the individual models\n",
    "torch.save(global_net.state_dict(), '/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/X-ray/Trained Models/V1/global_net_xray_v1.pth')\n",
    "torch.save(dad_head.state_dict(), '/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/X-ray/Trained Models/V1/dad_head_xray_v1.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_module = MVTecInferenceDataModule(\n",
    "    test_file_paths, \n",
    "    batch_size=1, \n",
    "    num_workers=0, \n",
    "    caching_strategy='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display inference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure batch_size in test_data_module is equal to BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE=4\n",
    "\n",
    "images = next(iter(test_data_module.test_dataloader()))\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 10))\n",
    "\n",
    "for i in range(4):\n",
    "  ax[i].imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that AnomalyDetector creates correct patches and binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_detector = AnomalyDetector(None, None, None, none=True)\n",
    "patches, binary_masks = anomaly_detector.create_patches_and_masks(images[0])\n",
    "\n",
    "# Reverse the normalization process done by data module\n",
    "# to avoid the following error:\n",
    "# WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverses the normalization in-place\n",
    "    return tensor.clamp(0, 1)  # Ensures the pixel values are within [0, 1]\n",
    "\n",
    "# Denormalize\n",
    "for i, patch in enumerate(patches):\n",
    "  patches[i] = denormalize(patch)\n",
    "\n",
    "# Plot image\n",
    "image = images[0].squeeze(0)\n",
    "plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()\n",
    "  \n",
    "# Plot all (overlapping) patches\n",
    "fig1, ax1 = plt.subplots(20, 20, figsize=(20, 20))\n",
    "for j in range(20):\n",
    "  for i in range(20):\n",
    "    ax1[j][i].imshow(patches[i+j*20].squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "    ax1[j][i].axis('off')\n",
    "\n",
    "# Plot all (overlappin) binary masks\n",
    "fig2, ax2 = plt.subplots(20, 20, figsize=(20, 20))\n",
    "for j in range(20):\n",
    "  for i in range(20):\n",
    "    ax2[j][i].imshow(binary_masks[i+j*20].squeeze(0).cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    #ax2[j][i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Anomaly Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is either 'cuda' if a GPU is available, otherwise 'cpu'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load all state dictionary from Google Drive\n",
    "local_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/LocalNet/Fine-tuning/X-ray/Trained Models/V1/local_net_finetuned_xray_v1.pth', map_location=device)\n",
    "global_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/X-ray/Trained Models/V1/global_net_xray_v1.pth', map_location=device)\n",
    "dad_state_dict = torch.load('/content/drive/MyDrive/AnomalyDetection/GlobalNet_DAD/X-ray/Trained Models/V1/dad_head_xray_v1.pth', map_location=device)\n",
    "\n",
    "# Initialise all networks\n",
    "local_net = LocalNet().to(device)\n",
    "global_net = GlobalNet().to(device)\n",
    "dad_head = DADHead().to(device)\n",
    "\n",
    "# Update all network's state dictionaries\n",
    "local_net.load_state_dict(local_state_dict)\n",
    "global_net.load_state_dict(global_state_dict)\n",
    "dad_head.load_state_dict(dad_state_dict)\n",
    "\n",
    "anomaly_detector = AnomalyDetector(local_net, global_net, dad_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[3]\n",
    "anomaly_score_map = anomaly_detector.detect_anomalies(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Apply a Gaussian blur to the anomaly map\n",
    "smoothed_anomaly_map = gaussian_filter(anomaly_score_map, sigma=5)  # You can adjust the sigma value\n",
    "\n",
    "# Visualize the smoothed anomaly map\n",
    "plt.imshow(smoothed_anomaly_map, cmap='jet')  # Using the same colormap\n",
    "plt.colorbar()  # To see the range of values on the smoothed anomaly score map\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()\n",
    "\n",
    "# Convert to numpy array and transpose to HWC format for image display\n",
    "image_np = image.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
    "\n",
    "# Normalize the resized anomaly map to the range [0, 1]\n",
    "normalized_anomaly_map = (smoothed_anomaly_map - np.min(smoothed_anomaly_map)) / (np.max(smoothed_anomaly_map) - np.min(smoothed_anomaly_map))\n",
    "\n",
    "# Apply a colormap to the normalized anomaly map\n",
    "colormap = plt.cm.jet\n",
    "normed_data = Normalize(0, 1)(normalized_anomaly_map)\n",
    "mapped_data = colormap(normed_data)\n",
    "\n",
    "# Convert the RGBA image to an RGB image\n",
    "mapped_data_rgb = (mapped_data[..., :3] * 255).astype(np.uint8)\n",
    "\n",
    "# Overlay the colored anomaly map onto the original image\n",
    "blended_image = Image.blend(image_pil.convert(\"RGBA\"), Image.fromarray(mapped_data_rgb).convert(\"RGBA\"), alpha=0.48)\n",
    "\n",
    "# Show the blended image\n",
    "plt.imshow(blended_image)\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
